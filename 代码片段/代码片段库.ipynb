{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连接 PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7d6109058d7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m connection = psycopg2.connect(user = \"username\",\n\u001b[0;32m      5\u001b[0m                                   \u001b[0mpassword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"password\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "connection = psycopg2.connect(user = \"username\",\n",
    "                                  password = \"password\",\n",
    "                                  host = \"127.0.0.1\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"dn_name\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "test_query = \"\"\"SELECT subject_id, hadm_id, admittime, dischtime, admission_type, diagnosis\n",
    "FROM admissions\n",
    "\"\"\"\n",
    "\n",
    "test = pd.read_sql_query(test_query, connection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表单导出数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5a6bbab5e556>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \"\"\"\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## 从单表中获取数据\n",
    "## 使用 WHERE 和 HAVING 语句进行过滤\n",
    "## 用 order_by_list 字段进行降序排序\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"SELECT [DISTINCT] <select_column_list> [AS <alias_name>]\n",
    "FROM <table_name>\n",
    "WHERE <where_condition_1> AND <where_condition_2> OR <where_condition_3>\n",
    "HAVING <having_condition>\n",
    "ORDER BY <order_by_list> [DESC]\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 WHERE 语句过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3515bbe846c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \"\"\"\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## 从单表中获取数据\n",
    "## 使用 WHERE 语句进行过滤，包含了 IN 包含关系，和使用 LIKE 做模式匹配\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"SELECT [DISTINCT] <select_column_list> [AS <alias_name>]\n",
    "FROM <table_name>\n",
    "WHERE column_name [NOT] IN ( value_1, value_2, ...,value_n)\n",
    "AND column_name BETWEEN value_1 AND value_2\n",
    "OR column_name LIKE 'string'\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 HAVING 语句过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 从单表中获取数据\n",
    "## 使用 HAVING 语句作过滤，包含聚合函数\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"SELECT [DISTINCT] <select_column_list> [AS <alias_name>]\n",
    "FROM <table_name>\n",
    "HAVING [aggregation function] = value_1\n",
    "AND [aggregation_function] = value_2\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取出前N条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 从单表中获取数据\n",
    "## 使用 WHERE 和 HAVING 语句进行过滤\n",
    "## 用 order_by_list 字段进行降序排序\n",
    "## 取出头部的N条数据\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"SELECT [DISTINCT] <select_column_list> [AS <alias_name>]\n",
    "FROM <table_name>\n",
    "WHERE <where_condition_1> AND <where_condition_2> OR <where_condition_3>\n",
    "HAVING <having_condition>\n",
    "ORDER BY <order_by_list> [DESC]\n",
    "LIMIT <selected N>\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多表导出数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 从多表中获取数据\n",
    "## 使用 INNER JOIN 从两表中获取数据\n",
    "## 使用 WHERE 和 HAVING 语句进行过滤\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"SELECT [DISTINCT] <select_column_list>\n",
    "FROM <left_table>\n",
    "<join_type> JOIN <right_table>\n",
    "ON <join_condition>  \n",
    "WHERE <where_condition>\n",
    "HAVING <having_condition>\n",
    "ORDER BY <order_by_list> DESC\n",
    "LIMIT <limit_number>\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用聚合函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 从多表中获取数据\n",
    "## 使用 INNER JOIN 从两表中获取数据\n",
    "## 使用 WHERE 和 HAVING 语句进行过滤\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"SELECT [aggregation function] (<column_name>)\n",
    "FROM <left_table>\n",
    "<join_type> JOIN <right_table>\n",
    "ON <join_condition>\n",
    "WHERE <where_condition>\n",
    "GROUP BY <group_by_all_the_left_columns>\n",
    "[HAVING <having_condition>]\n",
    "[ORDER BY <order_by_list>]\n",
    "[LIMIT <limit_number>]\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Subquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 如果SQL Query过长过复杂，可把Subquery的结果进行调用\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "\n",
    "query_string = \"\"\"--subquery作为一张表\n",
    "SELECT <column_list> FROM ( \n",
    "  SELECT <column_list> FROM table_name\n",
    ") AS alias_table_name\n",
    "\"\"\"\n",
    "query_string_1 = \"\"\"--subquery作为一系列值\n",
    "SELECT <column_list> FROM <table_name>\n",
    "WHERE <column_name> IN (\n",
    "  SELECT <column_name> FROM <table_name> WHERE  <where_condition>\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用With语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 如果SQL Query过长过复杂，可以将子查询的结果定义为表变量在后期复用\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"WITH table_variable_1 AS (\n",
    "  <SELECT query>\n",
    "),\n",
    "\n",
    "table_variable_2 AS (\n",
    "  SELECT * FROM table_variable_1;\n",
    ")\n",
    "\n",
    "SELECT * FROM table_variable_2；\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用条件表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 条件语句\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"-- 通用条件表达式 (类似if-else)\n",
    "CASE  \n",
    "WHEN condition1 THEN result1 -- i.e WHEN count > 5 THEN 1\n",
    "WHEN condition2 THEN result2 -- i.e. WHEN name = 'elle' THEN 'ELLE'\n",
    "[...]\n",
    "[ELSE result_n]\n",
    "END \n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看数据库中所有表名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 查看数据库中所有表名\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"\n",
    "SELECT * FROM pg_tables\n",
    "WHERE schemaname <> 'pg_catalog' AND schemaname <> 'information_schema'; \n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 窗口函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 窗口函数\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"SELECT <<column_name>,\n",
    "       window_func() OVER ( [PARTITION BY xx] [ORDER BY xx] )\n",
    "FROM <table_name>\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看表内字段类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 查看表内字段类型\n",
    "## conn 为 postgreSQL 的数据库连接对象\n",
    "query_string = \"\"\"\n",
    "select column_name, data_type \n",
    "from information_schema.columns \n",
    "where table_name = <table_name>\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query_string, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 描述性统计信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 列的分位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# set columns type\n",
    "my_df['col'] = my_df['col'].astype(np.float64)\n",
    "\n",
    "# computations for 4 quantiles : quartiles\n",
    "bins_col = pd.qcut(my_df['col'], 4)\n",
    "bins_col_label = pd.qcut(my_df['col'], 4).labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移动平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5d586c001751>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ret = np.cumsum(np.array(X), dtype=float)\n",
    "ret[w:] = ret[w:] - ret[:-w]\n",
    "result = ret[w - 1:] / w\n",
    "\n",
    "# X: array-like\n",
    "# window: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多重聚合（组函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns settings\n",
    "grouped_on = 'col_0'  # ['col_0', 'col_2'] for multiple columns\n",
    "aggregated_column = 'col_1'\n",
    "\n",
    "### Choice of aggregate functions\n",
    "## On non-NA values in the group\n",
    "## - numeric choice :: mean, median, sum, std, var, min, max, prod\n",
    "## - group choice :: first, last, count\n",
    "# list of functions to compute\n",
    "agg_funcs = ['mean', 'max']\n",
    "\n",
    "\n",
    "# compute aggregate values\n",
    "aggregated_values = my_df.groupby(grouped_on)[aggregated_columns].agg(agg_funcs)\n",
    "\n",
    "# get the aggregate of group\n",
    "aggregated_values.ix[group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 皮尔逊相关系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "matrix = np.transpose(np.array(X))\n",
    "np.corrcoef(matrix[0], matrix[1])[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在聚合的dataframe上使用用户定义方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top n in aggregate dataframe\n",
    "def top_n(group_df, col, n=2):\n",
    "    bests = group_df[col].value_counts()[:n]\n",
    "    return bests\n",
    "\n",
    "# columns settings\n",
    "grouped_on = 'col_0'\n",
    "aggregated_column = 'col'\n",
    "\n",
    "grouped = my_df.groupby(grouped_on)\n",
    "groups_top_n = grouped.apply(top_n, aggregated_column, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.std(np.array(X))\n",
    "\n",
    "# 协方差\n",
    "my_df.cov()\n",
    "\n",
    "# 方差\n",
    "np.var(np.array(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以列值排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# my_df 是pandas dataframe\n",
    "my_df['col_0'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.median(np.array(X))\n",
    "#平均数\n",
    "np.average(np.array(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些列的单一属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df[\"col\"].max() # [[\"col_0\", \"col_1\"]] 多字段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以频率排序降序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# my_df 是pandas dataframe\n",
    "my_df['col_0'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K平均数算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(k).fit(np.array(X))\n",
    "result = k_means.labels_\n",
    "label = result.tolist()\n",
    "return label, k, k_means.cluster_centers_.tolist(), k_means.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 所有列单一属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户定义方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns settings\n",
    "grouped_on = ['col_0']\n",
    "aggregated_columns = ['col_1']\n",
    "\n",
    "def my_func(my_group_array):\n",
    "    return my_group_array.min() * my_group_array.count()\n",
    "\n",
    "## list of functions to compute\n",
    "agg_funcs = [my_func] # could be many\n",
    "\n",
    "# compute aggregate values\n",
    "aggregated_values = my_df.groupby(grouped_on)[aggregated_columns].agg(agg_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组数据的基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns settings\n",
    "grouped_on = 'col_0'  # ['col_0', 'col_1'] for multiple columns\n",
    "aggregated_column = 'col_1'\n",
    "\n",
    "### Choice of aggregate functions\n",
    "## On non-NA values in the group\n",
    "## - numeric choice : mean, median, sum, std, var, min, max, prod\n",
    "## - group choice : first, last, count\n",
    "## On the group lines\n",
    "## - size of the group : size\n",
    "aggregated_values = my_df.groupby(grouped_on)[aggregated_column].mean()\n",
    "aggregated_values.name = 'mean'\n",
    "\n",
    "# get the aggregate of group\n",
    "aggregated_values.ix[group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大互信息数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.transpose(np.array(X)).astype(float)\n",
    "mine = MINE(alpha=0.6, c=15, est=\"mic_approx\")\n",
    "mic_result = []\n",
    "for i in matrix[1:]:\n",
    "    mine.compute_score(t_matrix[0], i)\n",
    "    mic_result.append(mine.mic())\n",
    "return mic_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据组的遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns settings\n",
    "grouped_on = 'col_0'  # ['col_0', 'col_1'] for multiple columns\n",
    "\n",
    "grouped = my_df.groupby(grouped_on)\n",
    "\n",
    "i = 0\n",
    "for group_name, group_dataframe in grouped:\n",
    "    if i > 10:\n",
    "        break\n",
    "    i += 1\n",
    "    print(i, group_name, group_dataframe.mean())  ## mean on all numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取类型字段的频数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看缺失情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in my_df.columns:\n",
    "    print(\"column {} 包含 {} 个缺失值\".format(col, my_df[col].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看非重复值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in my_df.columns:\n",
    "    print(\"column {} 中共有 {} 个unique value\".format(col, my_df[col].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新建列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns settings\n",
    "grouped_on = 'col_1'\n",
    "aggregated_column = 'col_0'\n",
    "\n",
    "### Choice of aggregate functions\n",
    "## On non-NA values in the group\n",
    "## - numeric choice : mean, median, sum, std, var, min, max, prod\n",
    "## - group choice : first, last, count\n",
    "my_df['aggregate_values_on_col'] = my_df.groupby(grouped_on)[aggregated_column].transform(lambda v: v.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用复杂方程设值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def complex_formula(col0_value, col1_value):\n",
    "    return \"%s (%s)\" % (col0_value, col1_value)\n",
    "\n",
    "my_df['new_col'] = np.vectorize(complex_formula)(my_df['col_0'], my_df['col_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户定义的聚合方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "   \n",
    "my_df['zscore_col'] = my_df.groupby(grouped_on)[aggregated_column].transform(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用自定义方程设值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a56d16bc5dd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmy_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'new_col'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'col_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'my_df' is not defined"
     ]
    }
   ],
   "source": [
    "def to_log(v):\n",
    "    try:\n",
    "        return log(v)\n",
    "    except:\n",
    "        return np.nan\n",
    "my_df['new_col'] = my_df['col_0'].map(to_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用一个值填补多列的缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df[['col_0', 'col_1']] = my_df[['col_0', 'col_1']].fillna(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用一个值填补一列的缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['col'] = my_df['col'].fillna(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除在指定列中带有缺失的数据条目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['col_0', 'col_1']\n",
    "records_without_nas_in_cols = my_df.dropna(subset=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用最后一个或缺失值的下一个填充数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - ffill : propagate last valid observation forward to next valid\n",
    "# - backfill : use NEXT valid observation to fill gap\n",
    "my_df['col'] = my_df['col'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用聚合得出的值填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_on = 'col_0' # ['col_1', 'col_1'] # for multiple columns\n",
    "\n",
    "### Choice of aggregate functions\n",
    "## On non-NA values in the group\n",
    "## - numeric choice : mean, median, sum, std, var, min, max, prod\n",
    "## - group choice : first, last, count\n",
    "def filling_function(v):\n",
    "    return v.fillna(v.mean())\n",
    "                    \n",
    "my_df['col'] = my_df.groupby(grouped_on)['col'].transform(filling_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除所有任何缺失值的数据条目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_without_nas = my_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算频率来转置表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = my_df.pivot_table(\n",
    "    rows=[\"make\"],\n",
    "    cols=[\"fuel_type\", \"aspiration\"],\n",
    "    margins=True     # add subtotals on rows and cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_record = my_df.iloc[1]  # !! get the second records, positions start at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 水平连接数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "two_dfs_hconcat = pd.concat([my_df, my_df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将dataframe的index重置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_no_index = my_df.reset_index()\n",
    "# my_df_no_index.index is now [0, 1, 2, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以元组(tuple)形式按行写入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "py_recipe_output = pd.read_csv(\"data.csv\")\n",
    "writer = py_recipe_output.get_writer()\n",
    "\n",
    "# t is of the form :\n",
    "#   (value0, value1, ...)\n",
    "\n",
    "for t in data_to_write:\n",
    "    writer.write_tuple(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据重构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot\n",
    "df3 = df2.pivot( index = 'Date', \n",
    "columns = 'Type', values = 'Value') #行变列\n",
    "\n",
    "#Pivot Table\n",
    "df4 = pd.pivot_table( df2,\n",
    "values='Value',\n",
    "index = 'Date', \n",
    "columns='Type'] #行变列\n",
    "\n",
    "#Stack/Unstack\n",
    "stacked = df5.stack( ) \n",
    "stacked.unstacked( )\n",
    "\n",
    "#Melt\n",
    "pd.melt(df2, id_vars=[\"Date\"], \n",
    "value_vars=[\"Type\",\"Value\"], \n",
    "value_name=\"Observations\") \n",
    "#将列变行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从列新建虚数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dummified_cols = pd.get_dummies(my_df['col']\n",
    "    # dummy_na=True # to include NaN values\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并有相同名称的列的两个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# my_df 是一个pandas dataframe\n",
    "merged = my_df.merge(my_df2,\n",
    "on='col', # ['col_0', 'col_1'] for many\n",
    "how=\"inner\",\n",
    "# suffixes=(\"_from_my_df\", \"_from_my_df2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并两个dataframe没有相同名称的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每次只合并两个dataframe\n",
    "# 合并方式 : 'left', 'right', 'outer', 'inner'\n",
    "import pandas as pd\n",
    "# my_df 是一个pandas dataframe\n",
    "merged = my_df.merge(my_df2,\n",
    "left_on=['col_0', 'col_1'],\n",
    "right_on=['col_2', 'col_3'],\n",
    "# suffixes=(\"_from_my_df\", \"_from_my_df2\")\n",
    "how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分组数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregation\n",
    "df2.groupby(\n",
    "by=['Data', 'Type']).mean()\n",
    "df4.groupby(level=0).sum()\n",
    "df4.groupby(level=0).agg\n",
    "({'a':lama x:sum(x)/len(x), \n",
    "'b':np.sum})\n",
    "\n",
    "#Transformation\n",
    "customSum=lamda x:(x+x%2)\n",
    "df4.groupby\n",
    "(level=0).transform(customSum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分簇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set columns type\n",
    "my_df['col'] = my_df['col'].astype(np.float64)\n",
    "\n",
    "# Computations\n",
    "bins = [0, 100, 1000, 10000, 100000] # 5 binned, labeled 0,1,2,3,4\n",
    "bins_col = pd.cut(my_df['col'], bins)\n",
    "bins_col_label = pd.cut(my_df['col'], bins).labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取元组(tuple)的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_dataset = pd.read_csv(\"path_to_dataset\")\n",
    "\n",
    "i = 0\n",
    "for my_row_as_tuple in my_dataset.iter_tuples():\n",
    "    if i > 10:\n",
    "        break\n",
    "    i += 1\n",
    "    print (my_row_as_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everywhere\n",
    "print(my_df.__doc__)\n",
    "print(my_df.sort.__doc__)\n",
    "\n",
    "# When using notebook : append a '?' to get help\n",
    "my_df?\n",
    "my_df.sort?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复制数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.unique() \n",
    "#返回唯一值\n",
    "df2.duplicated('Tyepe') \n",
    "#检查重复值\n",
    "df2.drop_duplicates(\n",
    "'Type', keep=last'') \n",
    "#丢弃重复值\n",
    "df.index.duplicated() \n",
    "#检查索引重复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用目标值的位置定位替换值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# my_df 是一个pandas dataframe\n",
    "my_df[]'col'].iloc[0] = new_value  # replacement for first record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按列里的值过滤或去除数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (my_df['col'] == value)\n",
    "\n",
    "# multiple values\n",
    "# cond = my_df['col'].isin([value1, value2])\n",
    "\n",
    "# null value\n",
    "# cond = my_df['col'].isnull()\n",
    "\n",
    "# exclude (negate condition)\n",
    "# cond = ~cond\n",
    "\n",
    "my_records = my_df[cond]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对多列数据排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = my_df.sort(['col_0', 'col_1'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将dataframe的index设为列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_with_col_index = my_df.set_index(\"col\")\n",
    "# my_df_col_index.index is [record0_col_val, record1_col_val, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据集加载成多个dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_dataset = pd.read_csv(\"data.csv\")\n",
    "\n",
    "for partial_dataframe in my_dataset.iter_dataframes(chunksize=2000):\n",
    "    # Insert here applicative logic on each partial dataframe.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以字典(dict)形式按行写入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "py_recipe_output = pd.read_csv(\"data.csv\")\n",
    "writer = py_recipe_output.get_writer()\n",
    "\n",
    "# d is of the form :\n",
    "#   {'col_0': value0, 'col_1': value1, ...}\n",
    "\n",
    "for d in data_to_write:\n",
    "    writer.write_row_dict(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用index获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_record = my_df.loc[label] # !! If the label in the index defines a unique record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用目标值的label定位替"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# my_df 是一个pandas dataframe\n",
    "my_df['col'].loc[my_label] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重命名所有列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.columns = ['new_col_0', 'new_col_1']  # needs the right number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置聚合方程进行交叉制表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉制表是统计学中一个将分类数据聚合成列联表的操作\n",
    "import pandas as pd\n",
    "# my_df 是一个pandas dataframe\n",
    "stats =  pd.crosstab(\n",
    "    rows=my_df[\"make\"],\n",
    "    cols=[my_df[\"fuel_type\"], my_df[\"aspiration\"]],\n",
    "    values=my_df[\"horsepower\"],\n",
    "    aggfunc='max',   # aggregation function\n",
    "    margins=True     # add subtotals on rows and cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "df3.loc[:, (df3>1).any()] \n",
    "#选列 其中任意元素>1\n",
    "df3.loc[:, (df3>1).all()] \n",
    "#选列 其中所有>1\n",
    "df3.loc[:, df3.isnull().any()] \n",
    "#选列 其中含空\n",
    "df3.loc[:,df3.notnull().all()] \n",
    "#选列 其中不含空\n",
    "\n",
    "df[(df.Country.isin(df2.Type))] \n",
    "#寻找相同元素\n",
    "df3.filter(items=[\"a\",\"b\"]) \n",
    "#根据值筛选\n",
    "df.select(lamda x: not x%5) \n",
    "#选特定元素\n",
    "\n",
    "s.where(s>0) \n",
    "#数据分子集\n",
    "\n",
    "df6.query('second >first') \n",
    "#query 数据结构\n",
    "\n",
    "\n",
    "#selecting\n",
    "df.set_index('Country') \n",
    "#设置索引\n",
    "df4 = df.reset_index() \n",
    "#重置索引\n",
    "df =df.rename(index = str, \n",
    "columns={\"Country\":\"cntry\", \n",
    "\"Capital\":\"cptl\",\"Population\":\"ppltn\"}) \n",
    "#重命名数据结构\n",
    "\n",
    "#Reindexing\n",
    "s2 = s.reindex(['a', 'c', 'd', 'e', 'b'])\n",
    "#Forward Filling\n",
    "df.reindex(range(4), \n",
    "method='ffill')\n",
    "#Backward Filling\n",
    "s3 = s.reindex(range(5), \n",
    "method='bfill')\n",
    "\n",
    "#MultiIndexing\n",
    "arrays = [np.array([1,2,3]), \n",
    "np.array([5,4,3])]\n",
    "df5 = pd.DataFrame(\n",
    "np.random.rand(3,2), index = arrays)\n",
    "tuples = list(zip( *arrays))\n",
    "index = pd.MultiIndex.from_tuples(\n",
    "tuples, names=['first', 'second'])\n",
    "df6 = pd.DataFrame(\n",
    "np.random.rand(3,2), index = index)\n",
    "df2.set_index([\"Data\",\"Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用某种法则替换列中的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rules = {\n",
    "    value: value1,\n",
    "    value2: value3,\n",
    "    'Invalid': np.nan  # replace by an true invalid value\n",
    "}\n",
    "\n",
    "my_df['col'] = my_df['col'].map(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遍历数据字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_dataset = pd.read_csv(\"path_to_dataset\")\n",
    "\n",
    "i = 0\n",
    "for my_row_as_dict in my_dataset.iter_rows():\n",
    "    if i > 10:\n",
    "        break\n",
    "    i += 1\n",
    "    print my_row_as_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 垂直连接数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dfs_vconcat = my_df1.append(my_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算频率进行交叉制表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉制表是统计学中一个将分类数据聚合成列联表的操作\n",
    "import pandas as pd\n",
    "# my_df 是一个pandas dataframe\n",
    "freqs = pd.crosstab(\n",
    "    rows=my_df[\"make\"],\n",
    "    cols=[my_df[\"fuel_type\"], my_df[\"aspiration\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge\n",
    "pd.merge(data1, data2, \n",
    "how='left', on='X1')\n",
    "pd.merge(data1, data2,\n",
    "how='right', on='X1')\n",
    "pd.merge(data1, data2, \n",
    "how='inner', on='X1')\n",
    "pd.merge(data1, data2, \n",
    "how='outer', on='X1')\n",
    "\n",
    "#join\n",
    "data1.join(data2, how='right')\n",
    "\n",
    "#concatenate\n",
    "#vertical\n",
    "s.append(s2)\n",
    "#horizontal/vertical\n",
    "pd.concat([s,s2], axis=1,\n",
    " keys=['One','Two'])\n",
    "pd.concat([data1, data2], \n",
    "axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重命名指定列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = my_df.rename(columns = {'col_1':'new_name_1', 'col_2':'new_name_2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用函数新增列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_minus_sign(v):\n",
    "    return str.replace('-', ' ', max=2)\n",
    "\n",
    "my_df['col'] = my_df['col'].map(remove_minus_sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从dataframe随机抽取N行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n = 10\n",
    "sample_rows_index = random.sample(range(len(my_df)), 10)\n",
    "my_sample = my_df.take(rows)\n",
    "my_sample_complementary = my_df.drop(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置聚合方程转置表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = my_df.pivot_table(\n",
    "    rows=[\"make\"],\n",
    "    cols=[\"fuel_type\", \"aspiration\"],\n",
    "    values=[\"horsepower\"],\n",
    "    aggfunc='max',   # aggregation function\n",
    "    margins=True     # add subtotals on rows and cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用boolean formula过滤或去除数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# single value\n",
    "cond1 = (my_df['col_0'] == value)\n",
    "cond2 = (my_df['col_1'].isin([value1, value2]))\n",
    "# boolean operators :\n",
    "# - negation : ~  (tilde)\n",
    "# - or : |\n",
    "# - and : &\n",
    "cond = (cond1 | cond2)\n",
    "my_records = my_df[cond]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Date'] = pd.to_datetime\n",
    "(df2['Data'])\n",
    "df2['Date'] = pd.date_range\n",
    "('2000-1-1', periods=6, freq ='M')\n",
    "dates = [datetime(2012,5,1), \n",
    "datetime(2012,5,2)]\n",
    "index = pd.DatetimeIndex(dates)\n",
    "index = pd.date_range\n",
    "(datetime(2012,2,1)), end, freq='BM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用dataframe的index新建列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['new_col'] = my_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## array 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Transposing Array\n",
    "I = np.transpose(b) #转置矩阵\n",
    "i.T #转置矩阵\n",
    "\n",
    "#Changing Array Shape\n",
    "b.ravel() #降为一维数组\n",
    "g.reshape(3,-2) #重组\n",
    "\n",
    "#Adding/Removing Elements\n",
    "h.resize((2,6)) #返回shape(2,6)\n",
    "np.append(h,g) #添加\n",
    "np.insert(a,1,5) #插入\n",
    "np.delete(a,[1]) #删除\n",
    "\n",
    "#Combining Arrays\n",
    "np.concatenate((a,d), axis=0) #连结\n",
    "np.vstack((a,b)) #垂直堆叠\n",
    "np.r_[e,f] #垂直堆叠\n",
    "np.hstack((e,f)) #水平堆叠\n",
    "np.column_stack((a,d)) #创建水平堆叠\n",
    "np.c_[a,d] ##创建水平堆叠\n",
    "\n",
    "#splitting arrays\n",
    "np.hsplit(a,3) #水平分离\n",
    "np.vsplit(c,2) #垂直分离\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数组进行排序\n",
    "a.sort()\n",
    "c.sort(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])   #创建数组\n",
    "b = np.array([(1.5,2,3), (4,5,6)], \n",
    "dtype=float)\n",
    "c = np.array([(1.5,2,3), (4,5,6)], \n",
    "[(3,2,1), (4,5,6) ] ], dtype=float)\n",
    "\n",
    "np.zeros((3,4))  #创建0数组\n",
    "np.ones((2,3,4), dtype=np.int16)  #创建1数组\n",
    "d = np.arrange(10,25,5)  #创建相同步数数组\n",
    "np.linspace(0,2,9)  #创建等差数组\n",
    "\n",
    "e = np.full((2,2), 7) #创建常数数组\n",
    "f = np.eye(2) #创建2x2矩阵\n",
    "np.random.random((2,2)) #创建随机数组\n",
    "np.empty((3,2)) #创建空数组\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#subsetting\n",
    "a[2] #选取数组第三个元素\n",
    "b[1,2] #选取2行3列元素\n",
    "\n",
    "#slicing\n",
    "a[0:2] #选1到3元素\n",
    "b[0:2,1] #选1到2行的2列元素\n",
    "b[:1] #选所有1行的元素\n",
    "c[1,...] #c[1,:,:]\n",
    "a[ : :-1]  #反转数组\n",
    "\n",
    "#Boolean Indexing\n",
    "a[a<2] #选取数组中元素<2的\n",
    "\n",
    "#Fancy Indexing\n",
    "b[[1,0,1,0], [0,1,2,0]]\n",
    "#选取[1,0],[0,1],[1,2],[0,0]\n",
    "b[[1,0,1,0][:, [0,1,2,0]]] \n",
    "#选取矩阵的一部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#arithmetic operation算术运算\n",
    "g = a - b\n",
    "np.subtract(a,b) #减法\n",
    "b+a\n",
    "np.add(b,a) #加法\n",
    "a / b\n",
    "np.divide(a,b) #除法\n",
    "a * b\n",
    "np.multiply(a,b) #乘法\n",
    "np.exp(b) #指数\n",
    "np.sqrt(b) #开方\n",
    "np.sin(a) #sin函数\n",
    "np.cos(b) #cos函数\n",
    "np.log(a) #log函数\n",
    "e.dot(f) #内积\n",
    "\n",
    "#Comparison比较\n",
    "a == b #元素\n",
    "a < 2 #元素\n",
    "np.array_equal(a,b) #数组 \n",
    "\n",
    "#Aggregate Functions 函数\n",
    "a.sum() #求和\n",
    "b.min() #最小值\n",
    "b.max(axis=0) #最大值数组列\n",
    "b.cumsum(axis=1) #元素累加和\n",
    "a.mean() #平均值\n",
    "b.median() #中位数\n",
    "a.corrcoef() #相关系数\n",
    "np.std(b) #标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape #数组维度\n",
    "len(a) #数组长度\n",
    "b.ndim #数组维度数量\n",
    "e.size #数组元素数量\n",
    "b.dtype #元素数据类型\n",
    "b.dtype.name #数据类型名\n",
    "b.astype(int) #改变数组类型\n",
    "\n",
    "#asking for help更多信息\n",
    "np.info(np.ndarray.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int64 #64位整数\n",
    "np.float32 #标准双精度浮点\n",
    "np.complex #复杂树已浮点128为代表\n",
    "np.bool #true&false\n",
    "np.object #python object\n",
    "np.string_ #固定长度字符串\n",
    "np.unicode_ #固定长度统一码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(my_array) #打印数组\n",
    "\n",
    "#saving &Loading on disk保存到磁盘\n",
    "np.save('my_array', a) \n",
    "np.savez('array.npz', a, b)\n",
    "np.load('my_array.npy')\n",
    "\n",
    "#saving &Loading Text files保存到文件\n",
    "np.loadtxt(\"my file.txt\")\n",
    "np.genfromtxt(\"my_file.csv\", delimiter=',')\n",
    "np.savetxt(\"marry.txt\", a, delimiter=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#复制数组\n",
    "h = a.view() \n",
    "np.copy(a)\n",
    "h = a.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors\n",
    "plt.plot(x, x, x, x**2, x, x**3)\n",
    "ax.plot(x, y, alpha = 0.4)\n",
    "ax.plot(x, y, c='k')\n",
    "fig.colorbar(im, orientation='horizontal')\n",
    "im = ax.imshow(img, cmap='seismic')\n",
    "\n",
    "#markers\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y,marker='.')\n",
    "ax.plot(x,y,marker='o')\n",
    "\n",
    "#linestyles\n",
    "plt.plot(x,y,linewidth=4.0)\n",
    "plt.plot(x,y,ls='solid')\n",
    "plt.plot(x,y,ls='--')\n",
    "plt.plot(x,y,'--',x**2,y**2,'-.')\n",
    "plt.setp(lines, color='r',linewidth=4.0)\n",
    "\n",
    "#Text & Annotations\n",
    "ax.text(1,-2.1, \n",
    "'Example Graph', style='italic')\n",
    "ax.annotate(\"Sine\", xy=(8,0),\n",
    "xycoords='data',xytext=(10.5,0), \n",
    "textcoords='data', arrowprops=\n",
    "dict(arrowstyle=\"->\",connectionstyle=\"arc3\"),)\n",
    "\n",
    "#mathtext\n",
    "plt.title(r'$sigma_i=15$',\n",
    " fontsize=20)\n",
    "\n",
    "#Limits,Legends&Layouts\n",
    "#Limites&Autoscaling\n",
    "ax.margins(x=0.0, y=0.1) \n",
    "#Add padding to a plot\n",
    "ax.axis('equal') \n",
    "#Set the aspect ratio of the plot to 1\n",
    "ax.set(xlim=[0,10.5],ylim=[-1.5,1.5]) \n",
    "#Set limits for x- and y-axis\n",
    "ax.set_xlim(0,10.5) \n",
    "#Set limits for x-axis\n",
    "\n",
    "#Legends\n",
    "ax.set(title='An Example Axes',\n",
    "ylabel='Y-Axis',xlabel='X-Axis') \n",
    "#Set a title and x- and y-axis labels\n",
    "ax.legend(loc='best') \n",
    "#No overlapping plot elements\n",
    "\n",
    "#Ticks\n",
    "ax.xaxis.set(ticks=range(1,5),\n",
    "ticklabels=[3,100,-12,\"foo\"]) \n",
    "#Manually set x-ticks\n",
    "ax.tick_params(axis='y',\n",
    "direction ='inout',length=10) \n",
    "#Make y-ticks longer and go in and out\n",
    "\n",
    "#Subplot Spacing\n",
    "fig3.subplots_adjust(wspace=0.5, \n",
    "hspace=0.3,left=0.125,right=0.9,\n",
    "top=0.9,bottom=0.1) \n",
    "#Adjust the spacing between subplots\n",
    "fig.tight_layout() \n",
    "#Fit subplots in to the figure area\n",
    "\n",
    "#Axis Spines\n",
    "ax1.spines['top'].set_visible(False) \n",
    "#Make the top axis line for a plot invisible\n",
    "ax1.spines['bottom'].set_position(('outward',10)) \n",
    "#Move the bottom axis line outward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Axisgrid Objects\n",
    "g.despine( left = True) \n",
    "#remove left spine\n",
    "g.set_ylabels( \"Survived\") \n",
    "#set the labels of the y-axis\n",
    "g.set_xticklabels( rotation = 45) \n",
    "#set the tick labels for x\n",
    "g.set_axis_labels( \"Survived\", \"Sex\")\n",
    " #set the axis labels\n",
    "h.set( xlim = (0,5), ylim = (0,5), \n",
    "xticks = [0,2.5,5], yticks =[0,2.5,5] )\n",
    "#set the limit and ticks of the x- and y-axis\n",
    "\n",
    "#Plot\n",
    "plt.title( \"A title\" ) \n",
    "#add plot title\n",
    "plt.ylabel( \"Survived\" ) \n",
    "#adjust the label of the y-axis\n",
    "plt.xlabel( \"Sex\" ) \n",
    "#adjust the label of the x-axis\n",
    "plt.ylim(0,100)\n",
    " #adjust the limits of the y-axis\n",
    "plt.xlim(0,10) \n",
    "#adjust the limits of the x-axis\n",
    "pot.setp(ax, yticks =[0,5]) \n",
    "#adjust a plot property\n",
    "plt.tight_layout() \n",
    "#adjust subplot params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 绘制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#axis grids\n",
    "g = sns.FacetGrid(titanic, \n",
    "col = \"survived\", row = \"sex\" ) \n",
    "#subplot grid\n",
    "g = g.map( pot.hist, \"age\")\n",
    "\n",
    "sns.factorplot( x = \"pclass\",\n",
    "y = \"survived\", hue = \"sex\", \n",
    "data = titanic) # categorical plot\n",
    "sns. lmplot( x = \"sepal_width\",\n",
    " y = \"sepal_length\", hue = \"species\", \n",
    "data = iris) \n",
    "#plot data and regression model\n",
    "\n",
    "h = sns.PairGrid( iris ) \n",
    "#subplot grid for plotting pairwise relationships\n",
    "h = h.map( pot.scatter) \n",
    "sns.pairplot( iris ) \n",
    "#plot pairwise bivariate distributions\n",
    "i = sns.JointGrid( x = \"x\",\n",
    " y = \"y\", data= data )\n",
    " #grid for bivariate plot with marginal\n",
    "i = i.plot( sns.regplot, sns.distplot )\n",
    "sns.jointplot( \"sepal_length\", \n",
    "\"sepal_width\", data = iris, kind = 'kde')\n",
    "#plot bivariate distribution\n",
    "\n",
    "\n",
    "#Categorical Plots\n",
    "\n",
    "#Scatterplot\n",
    "sns.stripplot( x = \"species\", \n",
    "y = \"petal_length\", data = iris)\n",
    "sns.swarmplot( x = \"species\", \n",
    "y = \"petal_length\", data = iris)\n",
    "\n",
    "#bar chart\n",
    "sns.barplot( x = \"sex\", y = \"survived\",\n",
    " hue = \"class\", data = titanic)\n",
    "\n",
    "#count plot\n",
    "sns.countplot( x = \"deck\", \n",
    "data = titanic, palette = \"Greens_d\")\n",
    "\n",
    "#Point Plot\n",
    "sns.pointplot( x = \"class\", y = \"survived\",\n",
    "hue = \"sex\", data = titanic, \n",
    "palette ={ \"male\": \"g\", \"female\": \"m\"}, \n",
    "markers = [ \"^\",\"o\"], linestyles = [\"-\",\"--\"])\n",
    "\n",
    "#Boxplot\n",
    "sns.boxplot( x = \"alive\", y = \"age\",\n",
    "hue = \"adult_male\", data = titanic)\n",
    "sns.boxplot( data = iris, orient = \"h\")\n",
    "\n",
    "#Violinplot\n",
    "sns.violinplot( x = \"age\", y = \"sex\",\n",
    " hue = \"survived\", data = titanic ) \n",
    "\n",
    "\n",
    "#regression plots\n",
    "sns.regplot(x=\"sepal_width\", \n",
    "y =\"sepal_length\", data=iris, ax=ax)\n",
    "\n",
    "#distribution plots\n",
    "plot = sns.displot(data.y, \n",
    "kde=False, color=\"b\")\n",
    "\n",
    "#Matrix plots\n",
    "sns.heatmap(uniform_data, \n",
    "vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建图像绘制窗口\n",
    "f, ax = ply.subplots( figsize = ( 5,6 ) )\n",
    "\n",
    "#seaborn style\n",
    "sis.set( ) #重置\n",
    "sns.set_style( \"whitegrid\" )\n",
    "#设置matplotlib parameters\n",
    "sns.set_style( \"ticks\",\n",
    "( \"xtick.major.size\": 8, \n",
    "\"ytick.major.size\": 8 ) )\n",
    "sns.axes_style( \"whitegrid\" ) \n",
    "#返回一个参数指引\n",
    "\n",
    "#context functions\n",
    "sns.set_context( \"talk\" ) \n",
    "#设置内容为“talk”\n",
    "sns.set_context( \"notebook\",\n",
    " font_scale = 1.5, \n",
    "rc = ( \"lines.linewidth\": 2.5 ) ) \n",
    "#设置内容为“notebook”\n",
    "\n",
    "#color palette\n",
    "sns.set_palette( \"husl\",3 )\n",
    " #定义color palette\n",
    "sns.color_palette( \"husl\" )\n",
    "flatui = ( “#9b59b6”,\"#3498db\",\n",
    "\"#95a5a6\",\"#e74c3c\",\n",
    "\"#34495e\",\"#2ecc71\")\n",
    "sns.set_palette( flatui )\n",
    "#定义自己的color palette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除所有重复值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_records = my_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_result = preprocessing.Normalizer().fit_transform(np.array(X))\n",
    "result = raw_result.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除特定列中的重复数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# my_df 是一个pandas dataframe\n",
    "unique_records_for_cols = my_df.drop_duplicates(cols=['col_1', 'col_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多项式数据变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_result = preprocessing.PolynomialFeatures().fit_transform(np.array(X))\n",
    "result = raw_result.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二值化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_result = preprocessing.Binarizer(threshold=t).fit_transform(np.array(X))\n",
    "result = raw_result.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类型转数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label1 和 Label2 表示 类别\n",
    "target = pd.Series(map(lambda x: dict(Label1=1, Label2=0)[x], my_df.target_col.tolist()), my_df.index)\n",
    "my_df.target_col = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_result = preprocessing.StandardScaler().fit_transform(np.array(X))\n",
    "result = raw_result.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 伯努利分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_result = preprocessing.MinMaxScaler().fit_transform(np.array(X))\n",
    "result = raw_result.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础维度变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用栈变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = pivoted.stack(dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用拆堆栈进行变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = base.unstack(level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用melt降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt是一种降维的方法\n",
    "import pandas as pd\n",
    "rows = pd.melt(pivoted, id_vars=[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = rows.pivot(index=\"type\", columns=\"make\", values=\"qty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 递归特征消除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "matrix = np.array(X)\n",
    "target = np.array(target)\n",
    "temp = feature_selection.RFE(estimator=LogisticRegression(), n_features_to_select=n_features).fit(matrix, target)\n",
    "scores = temp.ranking_.tolist()\n",
    "indx = temp.support_.tolist()\n",
    "result = temp.transform(matrix).tolist()\n",
    "return scores, indx, result\n",
    "# X: array-like\n",
    "# target: array-like\n",
    "# n-features: int\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关系数选择法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "matrix = np.array(X)\n",
    "target = np.array(target)\n",
    "temp = feature_selection.SelectKBest(lambda X, Y: np.array(list(map(lambda x: abs(pearsonr(x, Y)[0]), X.T))), k=k).fit(matrix, target)\n",
    "scores = temp.scores_.tolist()\n",
    "indx = temp.get_support().tolist()\n",
    "result = temp.transform(matrix).tolist()\n",
    "return scores, indx, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方差选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import feature_selection\n",
    "\n",
    "matrix = np.array(X)\n",
    "temp = feature_selection.VarianceThreshold(threshold=t).fit(matrix)\n",
    "scores = [np.var(el) for el in matrix.T]\n",
    "indx = temp.get_support().tolist()\n",
    "result = temp.transform(matrix).tolist()\n",
    "return scores, indx, result\n",
    "\n",
    "# X: array-like\n",
    "# t: float\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 互信息选择法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minepy import MINE\n",
    "import numpy as np\n",
    "from sklearn import feature_selection\n",
    "\n",
    "matrix = np.array(X)\n",
    "target = np.array(target)\n",
    "def mic(x, y):\n",
    "    m = MINE()\n",
    "    m.compute_score(x, y)\n",
    "    return (m.mic(), 0.5)\n",
    "temp = feature_selection.SelectKBest(lambda X, Y: np.array(list(map(lambda x: mic(x, Y), X.T))).T[0], k=k).fit(matrix, target)\n",
    "scores = temp.scores_.tolist()\n",
    "indx = temp.get_support().tolist()\n",
    "result = temp.transform(matrix).tolist()\n",
    "return scores, indx, result\n",
    "\n",
    "# X: array-like\n",
    "# target: array-like\n",
    "# k: int\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于惩罚值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "matrix = np.array(arr0)\n",
    "target = np.array(target)\n",
    "temp = feature_selection.SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit(matrix, target)\n",
    "indx = temp._get_support_mask().tolist()\n",
    "scores = get_importance(temp.estimator_).tolist()\n",
    "result = temp.transform(matrix).tolist()\n",
    "return scores, indx, result\n",
    "\n",
    "# X: array-like\n",
    "# target: array-like\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卡方检验法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "matrix = np.array(X)\n",
    "target = np.array(target)\n",
    "temp = feature_selection.SelectKBest(chi2, k=k).fit(matrix, target)\n",
    "scores = temp.scores_.tolist()\n",
    "indx = temp.get_support().tolist()\n",
    "result = temp.transform(matrix).tolist()\n",
    "return scores, indx, result\n",
    "\n",
    "# X: array-like\n",
    "# target: array-like\n",
    "# k: int\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import feature_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "matrix = np.array(X)\n",
    "target = np.array(target)\n",
    "temp = feature_selection.SelectFromModel(GradientBoostingClassifier()).fit(matrix, target)\n",
    "indx = temp._get_support_mask().tolist()\n",
    "scores = get_importance(temp.estimator_).tolist()\n",
    "result = temp.transform(matrix).tolist()\n",
    "return scores, indx, result\n",
    "\n",
    "# X: array-like\n",
    "# target: array-like\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA主成分分析算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "matrix = np.array(X)\n",
    "pca = PCA(n_components='mle', svd_solver='auto').fit(matrix)\n",
    "result = pca.transform(matrix)\n",
    "label = result.tolist()\n",
    "return label, pca.components_.tolist(), pca.explained_variance_.tolist(), pca.explained_variance_ratio_.tolist(), pca.mean_.tolist(), pca.noise_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 哑编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_result = preprocessing.OneHotEncoder().fit_transform(np.array(X))\n",
    "result = raw_result.tolist()\n",
    "\n",
    "# X: array-like\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE-t 分布邻域嵌入算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "matrix = np.array(X)\n",
    "t_sne = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "result = t_sne.fit(matrix)\n",
    "kl_divergence = result.kl_divergence_\n",
    "label = t_sne.fit_transform(matrix).tolist()\n",
    "\n",
    "return label, kl_divergence\n",
    "# X: array-like\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性判别分析法(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array(X)\n",
    "target = np.array(target)\n",
    "temp = LDA(n_components=n_components).fit(matrix, target)\n",
    "coef = temp.coef_\n",
    "mean = temp.means_\n",
    "priors = temp.priors_\n",
    "scalings = temp.scalings_\n",
    "xbar = temp.xbar_\n",
    "label = temp.transform(matrix).tolist()\n",
    "return label, coef.tolist(), mean.tolist(), priors.tolist(), scalings.tolist(), xbar.tolist()\n",
    "\n",
    "# X: array-like\n",
    "# target: array-like\n",
    "# n_components: int\n",
    "# http://scikit-learn.org/0.15/modules/generated/sklearn.lda.LDA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "clf.fit(X, y)\n",
    "\n",
    "# 数组 X shape:[n_samples, n_features]\n",
    "# 数组 y shape:[n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/linear_model.html\n",
    "from sklearn import linear_model\n",
    "\n",
    "lr =linear_model.LogisticRegression()  # penalty : str, ‘l1’ or ‘l2’, default: ‘l2’\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类树(Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# criterion = \"gini\" (CART) or \"entropy\" (ID3)\n",
    "clf = DecisionTreeClassifier(criterion = 'entropy' ,random_state = 0)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林(Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# criterion: str, 'gini' for Gini impurity (Default) and “entropy” for the information gain.\n",
    "clf = RandomForestClassifier(criterion='gini')  \n",
    "# X: array-like, shape = [n_samples, n_features]\n",
    "# y: array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
    "clf.fit(np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/dev/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "# Classifier Option 1: SVC()\n",
    "clf = svm.SVC()       # kernel = 'linear' or 'rbf' (default) or 'poly' or custom kernels; penalty C = 1.0 (default)\n",
    "# Option 2: NuSVC()\n",
    "# clf = svm.NuSVC() \n",
    "# Option 3: LinearSVC()\n",
    "# clf = svm.LinearSVC()     # penalty : str, ‘l1’ or ‘l2’ (default=’l2’)\n",
    "clf.fit(X, y)                # X shape = [n_samples, n_features], y shape = [n_samples] or [n_samples, n_outputs]\n",
    "\n",
    "# print(clf.support_vectors_) # get support vectors\n",
    "# print(clf.support_)         # get indeices of support vectors\n",
    "# print(clf.n_support_)       # get number of support vectors for each class\n",
    "\n",
    "mean_accuracy = clf.score(X,y)\n",
    "print(\"Accuracy: %.3f\"%(mean_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "# predict(test_set或者train_set) 值为你需要预测的数据集\n",
    "y_pred = gnb.fit(X, y).predict(X)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X.shape[0],(y != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树集成\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "adaboost_dt_clf = AdaBoostClassifier(\n",
    "                                    DecisionTreeClassifier(\n",
    "                                        max_depth=2,   # 决策树最大深度，默认可不输入即不限制子树深度\n",
    "                                        min_samples_split=20, # 内部结点再划分所需最小样本数，默认值为2，若样本量不大，无需更改，反之增大\n",
    "                                        min_samples_leaf=5    # 叶子节点最少样本数,默认值为1，若样本量不大，无需更改，反之增大\n",
    "                                        ),\n",
    "                                    algorithm=\"SAMME\", # boosting 算法 {‘SAMME’, ‘SAMME.R’}, 默认为后者\n",
    "                                    n_estimators=200,  # 最多200个弱分类器，默认值为50\n",
    "                                    learning_rate=0.8  # 学习率，默认值为1\n",
    "                                     )\n",
    "adaboost_dt_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT(梯度加速决策树)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbdt = GradientBoostingClassifier(max_depth=4,   # 决策树最大深度，默认可不输入，即不限制子树深度\n",
    "                                max_features=\"auto\",  # 寻找最优分割的特征数量，可为int,float,\"auto\",\"sqrt\",\"log2\",None:\n",
    "                                n_estimators=100 # Boosting阶段的数量，默认值为100。\n",
    "                                )\n",
    "gbdt.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "                n_clusters = 2, # 簇的个数，默认值为8\n",
    "                random_state=0  \n",
    "                ).fit(X)\n",
    "\n",
    "print(kmeans.labels_)\n",
    "print(\"K Clusters Centroids:\\n\", kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 keras 做分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "training_epochs = 200 #训练次数，总体数据需要循环多少次\n",
    "batch_size = 10  \n",
    "\n",
    "model = Sequential()\n",
    "input = X.shape[1]\n",
    "# 隐藏层128\n",
    "model.add(Dense(128, input_shape=(input,)))\n",
    "model.add(Activation('relu'))\n",
    "# Dropout层用于防止过拟合\n",
    "model.add(Dropout(0.2))\n",
    "# 隐藏层128\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# 没有激活函数用于输出层，二分类问题，用sigmoid激活函数进行变换，多分类用softmax。\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "# 使用高效的 ADAM 优化算法以，二分类损失函数binary_crossentropy，多分类的损失函数categorical_crossentropy\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=training_epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高度专门化的线性回归函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "stats.linregress(x, y=none)\n",
    "\n",
    "# x, y : array_like\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Statsmodels的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "results = sm.OLS(y, X).fit()\n",
    "\n",
    "# y: matrix\n",
    "# X: constant\n",
    "# http://www.statsmodels.org/dev/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一般的最小二乘多项式拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.polyfit(x, y, deg, rcond=None, full=False, w=None, cov=False)\n",
    "# x : array_like, shape (M,)\n",
    "# y : array_like, shape (M,) or (M, K)\n",
    "# deg : int\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyfit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用矩阵因式分解计算最小二程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import linalg\n",
    "\n",
    "linalg.lstsq(a, b, rcond=-1)\n",
    "\n",
    "# a : (M, N) array_like\n",
    "# b : {(M,), (M, K)} array_like\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.lstsq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更通用的最小二乘极小化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import optimize\n",
    "\n",
    "optimize.curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=(-inf, inf), method=None, jac=None, **kwargs)\n",
    "\n",
    "# f : callable\n",
    "# xdata : An M-length sequence or an (k,M)-shaped array for functions with k predictors\n",
    "# ydata : M-length sequence\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 keras 做线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential #顺序模型\n",
    "from keras.layers import Dense\n",
    "\n",
    "n_hidden_1 = 64 #隐藏层1的神经元个数\n",
    "n_hidden_2 = 64 #隐藏层2的神经元个数\n",
    "n_input = 13 #输入层的个数，也就是特征数\n",
    "n_classes = 1 #输出层的个数\n",
    "training_epochs = 200 #训练次数，总体数据需要循环多少次\n",
    "batch_size = 10  \n",
    "\n",
    "model = Sequential()#先建立一个顺序模型\n",
    "#向顺序模型里加入第一个隐藏层，第一层一定要有一个输入数据的大小，需要有input_shape参数\n",
    "#model.add(Dense(n_hidden_1, activation='relu', input_shape=(n_input,)))\n",
    "model.add(Dense(n_hidden_1, activation='relu', input_dim=n_input))\n",
    "model.add(Dense(n_hidden_2, activation='relu'))\n",
    "model.add(Dense(n_classes))\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['mae',r2])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=training_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
